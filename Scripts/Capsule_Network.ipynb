{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  In this script I am creating the advanced and latest model named CapsNet for the MNIST digits dataset. The model uses the dynamic routing by agreement algorithm.\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "# Technology used: TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with the usual utility cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# packages used for processing: \n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np\n",
    "\n",
    "# for pickling the data\n",
    "import cPickle as pickle\n",
    "\n",
    "# THE TensorFlow framework\n",
    "import tensorflow as tf\n",
    "# use the tensorflow's archived version of the MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Literature\n",
      "README.md\n",
      "Scripts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set a seed value for the script\n",
    "seed_value = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed_value) # set this seed for a device independant consistent behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../Data\" # the data path\n",
    "\n",
    "base_model_path = \"../Models\"\n",
    "\n",
    "# constant values for the script\n",
    "num_digits = 10 # This is defined. There are 10 labels for 10 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper parameters for tweaking.\n",
    "# ==================================================================================================================\n",
    "batch_size = 64 # 64 images in each batch\n",
    "no_of_epochs = 100\n",
    "checkpoint_factor = 5 # save model after 5 epochs\n",
    "primary_capsule_length = 8\n",
    "depthwise_count_of_primary_capsules = 32\n",
    "conv_lay_kernel_size = 9; conv_lay_stride = 1\n",
    "conv_lay_num_filters = 256\n",
    "prim_cap_lay_kernel_size = 9; prim_cap_lay_stride = 2\n",
    "digit_capsule_length = 16 # Can be tweaked later on\n",
    "ROUTING_ITERATIONS_COUNT = 4 # no. of routing by agreement iterations\n",
    "\n",
    "loss_lambda = 0.5\n",
    "recons_lambda = 0.0005\n",
    "no_of_epochs = 10\n",
    "# =================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../Data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../Data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../Data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../Data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist_data = input_data.read_data_sets(os.path.join(data_path, \"MNIST_data\"), reshape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = mnist_data.train.images; train_Y = mnist_data.train.labels\n",
    "dev_X = mnist_data.validation.images; dev_Y = mnist_data.validation.labels\n",
    "test_X = mnist_data.test.images; test_Y = mnist_data.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data shapes:  (55000, 28, 28, 1) (55000,)\n",
      "Development Data shapes:  (5000, 28, 28, 1) (5000,)\n",
      "Test Data shapes:  (10000, 28, 28, 1) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# print all the shapes:\n",
    "print \"Training Data shapes: \", train_X.shape, train_Y.shape\n",
    "print \"Development Data shapes: \", dev_X.shape, dev_Y.shape\n",
    "print \"Test Data shapes: \", test_X.shape, test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the total_train_examples:\n",
    "total_train_examples = train_X.shape[0] # first dimension is the batch dimension\n",
    "img_dim = train_X.shape[1] # second and third dimensions are height and width\n",
    "num_channels = train_X.shape[-1] # last dimension is the channels dimension. (Here it is 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Perform random checking of the images to verify if their sanity is maintained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run the following cell multiple times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAESCAYAAADdZ2gcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOfHNKsiJb1ZB/wAeajeoYKzorwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f52879c8c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' Randomized cell: Behaviour changes upon running multiple times '''\n",
    "\n",
    "random_index = np.random.randint(total_train_examples)\n",
    "\n",
    "# bring the random image from the training data\n",
    "random_image = np.squeeze(train_X[random_index], axis = -1)\n",
    "label_for_random_image = train_Y[random_index]\n",
    "\n",
    "# display this random image:\n",
    "plt.figure().suptitle(\"Image for digit: \" + str(label_for_random_image))\n",
    "plt.imshow(random_image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is a great tool that TensorFlow has provided under the examples package. The data setup becomes so easy especially for experimental notebooks. MNIST is one of the most used dataset and is regarded as the 'Drosophila' of Machine Learning by G. Hinton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Capsule Network part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# point to reset the graph if some error occurs below:\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the placeholders for the TensorFlow Computation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Input_data\"):\n",
    "    tf_input_images = tf.placeholder(tf.float32, shape=(None, img_dim, img_dim, num_channels), name=\"input_images\")\n",
    "    tf_input_labels = tf.placeholder(tf.int32, shape=(None,), name=\"input_labels\")\n",
    "    \n",
    "    # attach a summary op for the input_images\n",
    "    input_images_summary = tf.summary.image(\"input_images\", tf_input_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encode the labels into one_hot encoded format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"One_hot_encoder\"):\n",
    "    tf_one_hot_encoded_labels = tf.one_hot(tf_input_labels, depth=num_digits, name=\"one_hot_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One_hot_encoded labels:  Tensor(\"One_hot_encoder/one_hot_encoder:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check the dimensions of the tf_one_hot_encoded_labels\n",
    "print \"One_hot_encoded labels: \", tf_one_hot_encoded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the shape is as expected. So, we can now move forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the first two layers of the CapsNet architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the conv layer:\n",
    "with tf.name_scope(\"Conv1\"):\n",
    "    conv_lay_out = tf.layers.conv2d(tf_input_images, filters=conv_lay_num_filters, \n",
    "                        kernel_size=conv_lay_kernel_size, activation=tf.nn.relu, strides=conv_lay_stride,\n",
    "                    kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(seed=seed_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lay_1 output volume:  Tensor(\"Conv1/conv2d/Relu:0\", shape=(?, 20, 20, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check the shape of the lay_1 outputs\n",
    "print \"lay_1 output volume: \", conv_lay_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape is as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "giving due credit -> https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb <br>\n",
    "I have used the following function from above mentioned repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the squash function for the primary_caps layer:\n",
    "def squash(s, axis=-1, epsilon=1e-7, name=\"squash\"):\n",
    "    '''\n",
    "        Squash function as the block non-linearity.\n",
    "    '''\n",
    "    with tf.name_scope(name, default_name=\"squash\"):\n",
    "        # obtain the squared norm first\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis, keep_dims=True)\n",
    "        \n",
    "        # add a small epsilon to it to make the compuations stable\n",
    "        safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "        \n",
    "        # calculate the squash_factor term\n",
    "        squash_factor = squared_norm / (1. + squared_norm) # note that this term is already stable\n",
    "        \n",
    "        # calculate the unit vector\n",
    "        unit_vector = s / safe_norm\n",
    "        \n",
    "        # return the squashed output.\n",
    "        return squash_factor * unit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the primary caps layer:\n",
    "with tf.name_scope(\"PrimaryCaps\"):\n",
    "    primary_caps_lay_out = tf.layers.conv2d(conv_lay_out, kernel_size=prim_cap_lay_kernel_size, \n",
    "                                strides=prim_cap_lay_stride,\n",
    "                                filters=(primary_capsule_length * depthwise_count_of_primary_capsules),\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(seed=seed_value))\n",
    "    \n",
    "    # reshape the output volume in the form of capsules\n",
    "    _, h, w, _ = primary_caps_lay_out.get_shape() # extract the height and width of the original volume\n",
    "    primary_caps_lay_out = tf.reshape(primary_caps_lay_out, \n",
    "            shape=(-1, h * w * depthwise_count_of_primary_capsules, primary_capsule_length), name=\"capsule_maker\")\n",
    "    \n",
    "    # finally apply the squash function\n",
    "    primary_caps_lay_out = squash(primary_caps_lay_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primary_caps_lay output volume:  Tensor(\"PrimaryCaps/squash/mul:0\", shape=(?, 1152, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check the shape of the primary_caps_lay_out\n",
    "print \"primary_caps_lay output volume: \", primary_caps_lay_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the prev_layer_caps count\n",
    "_, no_of_primary_layer_caps, _ = primary_caps_lay_out.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = tf.shape(primary_caps_lay_out)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the weights required for the DigitCaps layer:\n",
    "with tf.variable_scope(\"DigitCaps/trainable_weights\"):\n",
    "    W = tf.get_variable(\"capsule_prediction_weights\", \n",
    "            shape=(1, no_of_primary_layer_caps, num_digits, digit_capsule_length, primary_capsule_length),\n",
    "            initializer=tf.contrib.layers.xavier_initializer(seed=seed_value))\n",
    "    \n",
    "#     # initialize the b values to zero.\n",
    "#     b_vals = tf.get_variable(\"log_priors\", \n",
    "#                              initializer = tf.zeros((1, no_of_primary_layer_caps, num_digits, 1)), \n",
    "#                              trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'DigitCaps/trainable_weights/capsule_prediction_weights:0' shape=(1, 1152, 10, 16, 8) dtype=float32_ref>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the digit_caps layer:\n",
    "''' This is an important cell! This implements the crux of the algorithm: \"The dynamic routing algorithm\" '''\n",
    "with tf.name_scope(\"DigitCaps\"):\n",
    "    # obtain the next_layer's predictions by using the defined weight matrices.\n",
    "    # tile the primary_caps_lay_out for the matrix multiplication:\n",
    "    W_tiled = tf.tile(W, multiples=[batch_size, 1, 1, 1, 1], name=\"W_tiler\")\n",
    "    \n",
    "    pclo_tiled = tf.tile(tf.expand_dims(tf.expand_dims(primary_caps_lay_out, axis=2), axis=-1), \n",
    "                         multiples=[1, 1, num_digits, 1, 1], name=\"pclo_tiler\")\n",
    "    \n",
    "    predicted_caps = tf.squeeze(tf.matmul(W_tiled, pclo_tiled), axis=[-1])\n",
    "        \n",
    "    def body(counter, routing_coeffs, output):\n",
    "        # obtain the softmax over the b_vals:\n",
    "        b_vals_attn_mask = tf.nn.softmax(routing_coeffs, dim=2)\n",
    "        \n",
    "        # attend over the predicted values followed by a squashing operation to obtain the result\n",
    "        output = squash(tf.reduce_sum(tf.multiply(predicted_caps, b_vals_attn_mask), axis=1))\n",
    "        \n",
    "        # tile the digit caps for calculating the measure of agreement.\n",
    "        dc_tiled = tf.tile(tf.expand_dims(output, axis=1), multiples=[1, no_of_primary_layer_caps, 1, 1])\n",
    "        \n",
    "        # the scalar product is calculated by multiplying the two matrices elementwise and summing over the last\n",
    "        # dimension\n",
    "        scalar_product = tf.reduce_sum(tf.multiply(dc_tiled, predicted_caps), axis=-1, keep_dims=True)\n",
    "        \n",
    "        # add this scalar product to the b_vals\n",
    "        routing_coeffs = routing_coeffs + scalar_product\n",
    "        \n",
    "        return (counter + 1, routing_coeffs, output)\n",
    "        \n",
    "    # execute the while loop:\n",
    "    _, _, digit_caps = tf.while_loop(\n",
    "                            lambda i, x, y: tf.less(i, ROUTING_ITERATIONS_COUNT),\n",
    "                            body,\n",
    "                            [tf.constant(0), \n",
    "                             tf.zeros((batch_size, no_of_primary_layer_caps, num_digits, 1), dtype=tf.float32), \n",
    "                             tf.zeros((batch_size, num_digits, digit_capsule_length), dtype=tf.float32)]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the loss and the optimizer for this operation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is also used from the same repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe_norm(s, axis=-1, epsilon=1e-7, keep_dims=False, name=None):\n",
    "    with tf.name_scope(name, default_name=\"safe_norm\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                     keep_dims=keep_dims)\n",
    "        return tf.sqrt(squared_norm + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the reconstruction module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'DigitCaps/while/Exit_2:0' shape=(?, 10, 16) dtype=float32>,\n",
       " <tf.Tensor 'One_hot_encoder/one_hot_encoder:0' shape=(?, 10) dtype=float32>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_caps, tf_one_hot_encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Reconstruction_module\"):\n",
    "    recons_input = tf.reduce_sum(digit_caps * tf.expand_dims(tf_one_hot_encoded_labels, axis=-1), axis=1)\n",
    "    \n",
    "    # use three fully connected layers for the reconstruction module\n",
    "    lay_1 = tf.layers.dense(recons_input, units=512, activation=tf.nn.relu, name=\"FC_layer_1\")\n",
    "    lay_2 = tf.layers.dense(lay_1, units=1024, activation=tf.nn.relu)\n",
    "    lay_3 = tf.layers.dense(lay_2, units=784, activation=tf.nn.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Trainer\"):\n",
    "    loss1 = tf.multiply(tf_one_hot_encoded_labels, tf.square(tf.maximum(0., 0.9 - safe_norm(digit_caps))))\n",
    "    loss2 = tf.multiply((1 - tf_one_hot_encoded_labels), tf.square(tf.maximum(0., safe_norm(digit_caps) - 0.1)))\n",
    "\n",
    "    # obtain the reconstruction loss:\n",
    "    flat_tf_input_images = tf.reshape(tf_input_images, shape=(-1, img_dim * img_dim))\n",
    "    recons_loss = tf.reduce_sum(tf.square(lay_3 - flat_tf_input_images))\n",
    "    \n",
    "    loss = tf.reduce_sum(loss1 + (loss_lambda * loss2)) + (recons_lambda * recons_loss)\n",
    "    \n",
    "    # define the Adam optimizer for optimization on this loss:\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    \n",
    "    # define the train_step for optimization:\n",
    "    train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "define the errands for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Errands\"):\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## write the session code for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-0d72d54fa3dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;31m# This is batch gradient descent: (We are running it only on first 512 images)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             _, cost = sess.run([train_step, loss], feed_dict={tf_input_images: batch_data_X, \n\u001b[1;32m---> 18\u001b[1;33m                                                               tf_input_labels: batch_data_Y})\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[1;34m\"epoch = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cost = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/botman/Programming/Platforms/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/botman/Programming/Platforms/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/botman/Programming/Platforms/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/botman/Programming/Platforms/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/botman/Programming/Platforms/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This is a temporary code and not the full blown:\n",
    "with tf.Session() as sess:\n",
    "    # initialize all the variables:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(no_of_epochs):\n",
    "        \n",
    "        # run through the batches of the data:\n",
    "        for batch in range(int(np.ceil(float(total_train_examples) / batch_size))):\n",
    "            start = batch * batch_size; end = start + batch_size\n",
    "            \n",
    "            # extract the relevant data:\n",
    "            batch_data_X = train_X[start: end]\n",
    "            batch_data_Y = train_Y[start: end]\n",
    "        \n",
    "            # This is batch gradient descent: (We are running it only on first 512 images)\n",
    "            _, cost = sess.run([train_step, loss], feed_dict={tf_input_images: batch_data_X, \n",
    "                                                              tf_input_labels: batch_data_Y})\n",
    "        print \"epoch = \", epoch, \"cost = \", cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
